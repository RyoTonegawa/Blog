# LoRA をゼロから理解するための下書き

PJでLoRAを利用することになったので、LLMのファインチューニング方法であるLoRAについて噛み砕いて解説を書きます。

---

## 今回のまとめ

- LoRAはLow-Rank Adaptationの略。
- LoRAはLLMを低コストでファインチューニングするための手法
- LLMで利用される重み行列をそのままファインチューニングするのではなく、元の重みをより小さな行列(低ランク行列/Low-Rank)で表現し、その行列(重み)だけを再学習する方法。

## そもそもLoRAとは？

- フル微調整は巨大な重み行列を全部更新するためコストが跳ね上がる。
- 行列は**線型独立な成分を抽出することでより小さな行列で近似できる**ため、「本当は高次元全部を動かさなくても、変化の本質は低次元で表せるのでは？」という素朴な直感が出発点。
- LoRA は上記のアイデアを利用し、既存の重み \(W_0\) を凍結したまま、**元の重みを再現した低ランク行列** \(\Delta W\) だけを学習するアダプタ方式。
- 元の重みの後段に追加で学習した層を追加し、元の出力を新しく再学習した方向へと変化させる。
こうすることで、元の重みが持つ汎化性能を生かしつつ、ファインチューンを低コストで行うことができる。

##　今回理解するLoRAの流れ

- 今回理解する内容はこれ。今はわからなくてもOK。
   ・$ W_0 x$：Attentionの元の重み(フルランク行列)
   ・$\Delta W x$：LoRAで学習する追加の重み
   ・$\frac{\alpha}{r}$：スケーリング係数
   ・$B$：LoRA で学習する低ランク分解のパラメータ
   ・$A$：LoRA で学習する低ランク分解のパラメータ
  \[
  y = W_0 x + \Delta W x, \quad
  \Delta W = \frac{\alpha}{r} B A,\quad \text{rank}(BA) = r \ll \min(d_\text{in}, d_\text{out})
  \]
   
## ランクとは何か

- ランク（rank）は「その行列に含まれる**独立な方向の本数**」。列（あるいは行）の線形結合で表せないベクトルの数を数えていると考えると分かりやすい。
- 例: 全ての列が「最初の列を倍にしただけ」なら独立な列は 1 本 → ランク 1。情報は 1 次元の直線上にしか存在しない。
- 行列のランクはベクトル空間が何次元の部分空間に潰れているかを教えてくれる指標であり、「自由度」「表現力」の上限を決める。
- LoRA では差分重みを「ランク \(r\) の行列 \(BA\)」に制限することで、巨大なフルランク行列を全部動かさなくても必要な方向だけ学習できるようにしている。


## SVD の手計算で"低ランクで近似できる"を理解する

`memo/SVD/README.md` では 2×2 行列を手計算で特異値分解し、ランク 1 近似まで作っている。初見殺しになりがちな SVD を次の 3 ステップで把握しよう。

1. \(W^\top W\) の固有値 → 特異値 \(\sigma_i = \sqrt{\lambda_i}\) を得る。
2. 固有ベクトルを正規化して右特異ベクトル \(v_i\) を求める。
3. \(u_i = (1/\sigma_i) W v_i\) で左特異ベクトルが手に入る。

ランク 1 近似 \(W_1 = \sigma_1 u_1 v_1^\top\) を作り、元の行列と比べることで「上位の特異値だけ残してもだいたい同じ」感覚を掴む。LoRA は学習中にこれをオンラインでやっているイメージ、と覚えておくと理解がスムーズ。

## 第 4 章: 具体的な行列例でランクを計算してみる


\[
W = \begin{bmatrix}
4 & 8 & 12\\
2 & 4 & 6\\
3 & 6 & 9\\
5 & 10 & 15
\end{bmatrix} \in \mathbb{R}^{4 \times 3}
\]
上記のような行列を見てみると以下のような特徴がわかる。

「実は列が全部 1 本の方向に並んでいるためランク 1 で十分」ということを行
- 一行目を1/2倍すると二行目に、3/4倍すると3行目、5/4倍すると４行目になる。
- 一列目を二倍すると二列目に、三倍すると三列目になる。
このことから、実はこの行列は一列のベクトルと一行のベクトルの掛け算で表せることに気づく。
こうすると一行三列と一列四行のベクトルでよくなるため、６つ分のパラメータを削減できる。
このことから、以下のことがわかる。

- **ランクは独立な列（行）の本数**。行列全体が 1 方向に縮んでいれば、データは低次元で表せる。
- ほんの少しノイズが乗っても、射影（最小二乗）でほぼ同じ方向に戻せる → LoRA の「重み差分はだいたい低ランクで書ける」という経験則につながる。

## 第 5 章: LoRA の式「ΔW = (α / r) BA」を一つずつ分解する

質問でもあった「BrA で一旦圧縮して戻す」イメージは、以下の 3 パーツに分けると腑に落ちる（詳細は `memo/特異値分解詳細実例解説.md` の後半）。

1. **A (r × d_in)** — 高次元入力を rank \(r\) のボトルネックへ射影。ここで情報の圧縮が起こる。
2. **スケール α / r** — 学習安定化のためのゲイン。r で割って正規化し、α で必要な大きさまで戻す。
3. **B (d_out × r)** — 圧縮された情報を出力次元へ持ち上げる。LoRA が本当に学習するのはこの部分。

これが SVD の \(U_r \Sigma_r V_r^\top\) と対応していると思えば、式の形だけで迷子にならない。

## 第 6 章: r（ランク）をどう決めるか

`memo/ΔWの計算方法について/README.md` にある通り、r は「特異値のどこまで残すか」を決めるノブ。

1. SVD で特異値 \(\sigma_i\) を並べ、フロベニウスノルムに対する累積寄与率
   \[
   E(r) = \frac{\sum_{i=1}^r \sigma_i^2}{\sum_{i} \sigma_i^2}
   \]
   を計算する。
2. 90% or 95% など保持したいエネルギー比率を決め、最小の r を選ぶ。
3. 実務ではレイヤーごとに特異値の減衰を見る。減衰が速い層は小さい r で十分、遅い層は r を大きく。

**なぜ特異値二乗和？**  
特異値の二乗は各直交方向のエネルギーであり、フロベニウスノルムと一致するため「これだけ情報を残している」と言えるから。エックハルト–ヤングの定理が保証してくれるので、他の指標より筋が良い。

## 第 7 章: LoRA をコードに落とし込むときの手順

1. **対象レイヤーを決める** — 多くは Transformer の `W_q, W_k, W_v, W_o` や FFN の線形層。
2. **ベース重み \(W_0\) を凍結** — Optimizer からパラメータを除外し、LoRA の \(A, B\) だけ学習可能にする。
3. **初期化** — \(A\) をランダム、\(B\) をゼロにしておく（初期は実質ノーオペ）。
4. **学習時** — フォワードで `W_0 x + (α/r) B A x` を計算し、逆伝播で \(A,B\) のみ更新。
5. **推論時** — \(\Delta W\) をマージしてもいいし、オンザフライで \(B A x\) を足してもいい。

LoRA を PyTorch で書く際の疑問（hook の貼り方、rank ごとのパラメータ数など）はこの章で扱う想定。

## 第 8 章: 例題ベースで LoRA を追体験する

- `memo/特異値分解詳細実例解説.md` の行列を LoRA の \(B, A\) に見立てて再構成する。
- 「rank 1 でも 80% のエネルギーを保てた」という数値から、実際のモデルでも r をケチれる理由を示す。
- さらにノイズ付き行列を射影して「だいたい合う」ことを体感する。

ここまでくると、LoRA が単なるハックではなく線形代数に裏付けられた手法だと実感できる。

## 第 9 章: ありがちな疑問と落とし穴

1. **r を大きくすればするほど良い？**  
   → 表現力は上がるがパラメータ数も増える。特異値の減衰を見て段階的に増減させるのが定石。
2. **学習が不安定になる理由は？**  
   → スケール α の設定、ベース重みを凍結し忘れている、LoRA を当てるレイヤー選定ミス等。
3. **推論時のレイテンシは？**  
   → \(\Delta W\) を事前にマージして配布する、または LoRA アダプタをオンデマンドで読み込むなど運用で解決。

## 第 10 章: 次に読むべきメモ一覧

- `memo/行列基礎/README.md` — 自由度・特異値・固有値の基礎まとめ
- `memo/SVD/README.md` — SVD を手計算で追うステップ
- `memo/特異値分解詳細実例解説.md` — 具体行列でランク計算と BrA の内訳を確認
- `memo/ΔWの計算方法について/README.md` — r の選び方、特異値二乗和を指標にする理由

---

この下書きを土台に、各章をブログ記事へ肉付けしていけば「数学初心者でも LoRA の何が嬉しいのか」をストーリーで伝えられる。必要に応じてコードスニペットや図を追加し、実装メモと行き来できるようリンクを張っていく予定。***
