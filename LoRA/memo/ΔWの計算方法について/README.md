# ΔW を低ランク化するときの \(r\) の決め方と計算フロー

LoRA では、学習中に得られる差分 \(\Delta W \in \mathbb{R}^{d_{\text{out}} \times d_{\text{in}}}\) を
\[
\Delta W \approx \frac{\alpha}{r} \, B A
\quad
(B \in \mathbb{R}^{d_{\text{out}} \times r},\ A \in \mathbb{R}^{r \times d_{\text{in}}})
\]
と分解し、**ボトルネック次元 \(r\)** で情報を伝える。ここでは、実際にどのように \(r\) を決め、数値的に求めるかをステップ形式で整理する。

## 1. フルサイズの \(\Delta W\) を用意する

例として、前段で扱った 2×2 行列を \(\Delta W_{\text{full}}\) とみなす。
\[
\Delta W_{\text{full}} =
\begin{bmatrix}
4 & 0 \\
3 & 5
\end{bmatrix}
\]

## 2. SVD を実行し、特異値列 \(\{\sigma_i\}\) を得る

\[
\Delta W_{\text{full}} = U \Sigma V^\top,
\quad
\Sigma = \operatorname{diag}(\sigma_1, \sigma_2)
\]

この例では
- \(\sigma_1 \approx 6.3249\)
- \(\sigma_2 \approx 3.1623\)

## 3. エネルギー（寄与率）を基準に \(r\) を選ぶ

フロベニウスノルムは \(\|\Delta W\|_F^2 = \sum_i \sigma_i^2\)。  
累積寄与率（保持エネルギー）は
\[
E(r) = \frac{\sum_{i=1}^{r} \sigma_i^2}{\sum_{i=1}^{\text{rank}} \sigma_i^2}
\]

例の数値:
- 合計エネルギー \(= 6.3249^2 + 3.1623^2 = 40 + 10 = 50\)
- \(r = 1\) なら \(E(1) = 40 / 50 = 0.8\)（80% を保持）
- \(r = 2\) なら \(E(2) = 1.0\)（100%）

**よくある決め方**
1. 「90%以上保持したい」などの閾値を決める。
2. 最小の \(r\) でその閾値を超えるまで累積する。
3. モデルの制約（VRAM、パラメータ予算）と照らし合わせて微調整する。

この例では 90% に達しないため、厳密には \(r=2\) が必要。ただし 80% で十分なら \(r=1\) を採用し、パラメータを半減できる。

## 4. \(B\) と \(A\) を構成する

トランケートした SVD から直接 \(B\) と \(A\) を組み立てる方法:
\[
B = U_r \Sigma_r^{1/2}, \qquad
A = \Sigma_r^{1/2} V_r^\top
\]

### \(r = 1\) の具体値
- \(U_1 = [0.4472,\ 0.8944]^\top\)
- \(V_1^\top = [0.7071,\ 0.7071]\)
- \(\Sigma_1^{1/2} = \sqrt{6.3249} \approx 2.5149\)

結果:
\[
B =
\begin{bmatrix}
1.125 \\
2.251
\end{bmatrix},
\quad
A =
\begin{bmatrix}
1.778 & 1.778
\end{bmatrix}
\]

これらから
\[
\hat{\Delta W} = B A =
\begin{bmatrix}
2.0 & 2.0 \\
4.0 & 4.0
\end{bmatrix}
\approx \Delta W_{\text{full}}
\]
となり、ランク 1 で 80% のエネルギーを保った近似が得られる。LoRA 実装では学習可能な \(B, A\) を初期化し、更新後に上のような構造に近づくと考えればよい。

## 5. 実務での \(r\) 選定チェックリスト

1. **レイヤーごとに SVD を取り、特異値の落ち方を観察**  
   早く減衰するレイヤー（上位数個の特異値だけが大きく、残りが急速に 0 へ近づく層）ほど、小さな \(r\) で十分。
2. **累積寄与率のカーブからしきい値を決定**  
   例: 95% を維持したい場合は \(\min r: E(r) \ge 0.95\)。
3. **パラメータ数と計算量を見積もる**  
   1 レイヤーあたりの追加パラメータは \(r(d_{\text{in}} + d_{\text{out}})\)。総計が予算内か確認。
4. **学習実験で微調整**  
   数値上十分でも精度が足りない場合、\(r\) を 1.5～2 倍に増やしてみる。

このように、\(r\) は「どれだけの特異値を残すか」を決める尺度であり、SVD によるスペクトル解析 → 累積寄与率 → 予算・精度とのトレードオフという流れで求めるのが基本になる。

## 6. 「特異値が早く減衰するレイヤー」とは？

- **定義**: 特異値列 \(\sigma_1 \ge \sigma_2 \ge \dots\) が指数的／急速に小さくなる層。上位 3～5 個で総エネルギーの大半（例: 95% 以上）が説明できる。
- **直感**: そのレイヤーは入力をほぼ少数の方向にだけ写像しており、残りの方向はほとんど 0 に押しつぶされている。
- **LoRA への示唆**: 必要な自由度（= \(r\)）は特異値の「有効本数」に近い。減衰が遅い層は高ランクで情報が分散しているため、より大きな \(r\) が必要。

視覚化すると、減衰の速い層は折れ線グラフが急激に落ちる「肘」型になり、遅い層はほぼ直線的に滑らかに下がる。肘の位置がそのまま推奨ランクの目安になる。

## 7. なぜ「特異値の 2 乗和」が情報量の指標になるのか

1. **フロベニウスノルムの分解性**  
   \[
   \|W\|_F^2 = \sum_{i,j} w_{ij}^2 = \sum_{k} \sigma_k^2
   \]
   という等式が成り立つ。すなわち、行列エネルギー（要素二乗和）は特異値平方和に等しい。
2. **直交基底でのエネルギー保存**  
   SVD は直交基底（\(U, V\)）で回転し、\(\Sigma\) で軸方向にスケーリングするだけなので、特異値を二乗した量が各直交方向の寄与を表す。
3. **低ランク近似の最適性**  
   エックハルト–ヤングの定理より、上位 \(r\) 個の特異値を残した \(W_r\) は Frobenius ノルム誤差を最小にする。従って \(\sum_{k=1}^r \sigma_k^2\) が最大限の「保持情報量」になる。

この性質により、特異値の 2 乗和を累積しながら「元の行列エネルギーを何 % 保っているか？」を評価するのが自然で、LoRA でも \(r\) の妥当性判断にそのまま利用される。
