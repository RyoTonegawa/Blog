# 特異値分解・低ランク近似の実例解説

## 1. サンプル重み行列

ここでは 4 個の入力特徴を 3 次元に写像する全結合層の重みを、次の実数行列で具体的に考える。

\[
W =
\begin{bmatrix}
4 & 8 & 12 \\
2 & 4 & 6 \\
3 & 6 & 9 \\
5 & 10 & 15
\end{bmatrix}
\in \mathbb{R}^{4 \times 3}
\]

- 各列は入力 3 次元の基底ベクトルに、各行は出力ニューロンが受け取る重みベクトルに対応するイメージ。
- \(\mathbb{R}^{4 \times 3}\) は「実数（\(\mathbb{R}\)）の要素を持つ 4 行 3 列の行列」を意味し、上の \(W\) のように 4 本の行ベクトルと 3 本の列ベクトルで構成される集合を指す。
- 行も列も互いに強く相関している（列 2 は列 1 の 2 倍、列 3 は列 1 の 3 倍）ため、この行列は実は秩 1 で完全に記述できる。

## 2. そもそもランク（秩）とは

- **定義**: 行列のランクは、その列（あるいは行）が張るベクトル空間の次元。直感的には「互いに独立な情報（方向）の数」。
- **幾何的な意味**: 3 次元空間にあっても、データが 1 本の直線上にしか存在しなければランク 1。埋め込まれている空間の次元よりずっと低い次元で情報を表せる。
- **線形写像としての意味**: ランクは写像が取り得る像の次元。入力の変化が出力に実際何種類の方向として現れるかを示す。

## 3. ランクの計算方法（例: 行基本変形）

行基本変形で行列を階段行列にすると、非ゼロ行の数がランクになる。上の行列に対し、以下の操作を行う。

1. \(R_2 \leftarrow R_2 - \tfrac{1}{2}R_1\)、\(R_3 \leftarrow R_3 - \tfrac{3}{4}R_1\)、\(R_4 \leftarrow R_4 - \tfrac{5}{4}R_1\)  
   \[
   \begin{bmatrix}
   4 & 8 & 12 \\
   0 & 0 & 0 \\
   0 & 0 & 0 \\
   0 & 0 & 0
   \end{bmatrix}
   \]
2. 以降の行も 0 なので、非ゼロ行は 1 本だけ → ランクは 1。

他にも以下の求め方がある。

- **正則な小行列のサイズを見る**: 最大で 1 次の小行列だけが非ゼロ行列式を持つためランク 1 と分かる。
- **特異値分解 (SVD)**: \(W = U \Sigma V^\top\) と分解し、0 でない特異値（\(\Sigma\) の対角成分）の個数がランク。実装上は `np.linalg.svd` や `torch.linalg.svd` で数値的に安定して求められる。

## 4. なぜ低ランクで元の重みを表現できるのか

### 4.1 外積 1 本で完全再現できる例

上の \(W\) は次の外積 1 本（ランク 1 行列）で厳密に表せる。

\[
B =
\begin{bmatrix}
4 \\ 2 \\ 3 \\ 5
\end{bmatrix},
\quad
A = \begin{bmatrix} 1 & 2 & 3 \end{bmatrix}
\quad \Rightarrow \quad
BA =
\begin{bmatrix}
4 \\ 2 \\ 3 \\ 5
\end{bmatrix}
\begin{bmatrix}
1 & 2 & 3
\end{bmatrix}
= W
\]

つまり、全 12 個の値を 7 個（\(B\) の 4 要素と \(A\) の 3 要素）で表現できたことになり、これが「低ランク＝情報圧縮」の最も単純な姿である。

### 4.2 少しノイズが乗ったとしても近似できる

実用上は完全なランク 1 ではなく、例えばタスクで得た差分が

\[
\tilde{W} =
\begin{bmatrix}
4.2 & 8.1 & 11.9 \\
2.1 & 4.0 & 6.1 \\
2.9 & 6.1 & 9.2 \\
5.1 & 10.2 & 15.1
\end{bmatrix}
\]

のように少しだけズレていることが多い。それでも、元の方向ベクトル \(B = [4,2,3,5]^\top\) に射影すれば、最小二乗の意味で最適な係数は

\[
a_j = \frac{B^\top \tilde{w}_j}{B^\top B}
\quad (j = 1,2,3)
\quad \Rightarrow \quad
[a_1, a_2, a_3] \approx [1.022,\, 2.032,\, 3.017]
\]

となり、\(\hat{W} = B[a_1, a_2, a_3]\) は各列に対して平均 0.1 程度の誤差で \(\tilde{W}\) を近似できる。  
これは **データ（＝重みの変化）がほぼ 1 次元の部分空間に乗っている** からであり、「低ランクで十分」という事実の現れである。

### 4.3 SVD の視点

- SVD では特異値 \(\sigma_1 \ge \sigma_2 \ge \dots\) の減衰が早いほど、上位 \(r\) 個だけ残した近似 \(W_r = U_r \Sigma_r V_r^\top\) で再現誤差が小さい。
- 上の例では \(\sigma_1 \gg \sigma_2, \sigma_3\) となり、\(\sigma_1\) だけ残せばほとんどの情報が保たれる。これが「なぜ低ランクで表現できるか？」の定量的指標になる。

## 5. 「BrA で一旦圧縮して戻す」とは？

LoRA でよく出てくる式

\[
\Delta W = \frac{\alpha}{r} \, B A
\]

を、文脈によっては \(B r A\) と略して表現することがある。意味は次の通り。

1. **\(A \in \mathbb{R}^{r \times d_{\text{in}}}\)（下り射）**  
   - 高次元の入力をランク \(r\) の低次元空間へ射影し、情報を圧縮する。
2. **スケーリング \( \alpha / r \)（または対角行列 \(R\)）**  
   - 勾配の大きさを調整するゲイン。しばしば「\(r\) で割って正規化し、\(\alpha\) で再スケールする」ため、\(B r A\) の \(r\) を「一旦縮めて戻す」プロセスと捉えることがある。
3. **\(B \in \mathbb{R}^{d_{\text{out}} \times r}\)（上り射）**  
   - 圧縮された情報を再び元の出力次元へ持ち上げる。ここで追加される自由度は \(d_{\text{out}} \times r\) 個だけ。

このように
- 「**前段 \(A\)** で次元を落とし（圧縮）」
- 「**ボトルネック（\(r\)）で情報を保持**」
- 「**後段 \(B\)** で元の次元に戻す」

という 3 ステップが、SVD の \(U_r \Sigma_r V_r^\top\) をオンラインに学習しているのと同じ構造になっている。サンプル行列に対しては \(r = 1\) で十分であり、\(B = [4,2,3,5]^\top\)、\(A = [1,2,3]\) がそのまま LoRA の更新方向に該当するイメージになる。

---

このように、具体的な数値行列でランクを計算し、低ランク分解がどのように情報を圧縮しつつ本質だけを保っているかを確認すると、LoRA の「BrA で圧縮して戻す」という発想が SVD に根ざしていることが腑に落ちる。
