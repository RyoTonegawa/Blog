# 特異値分解とLoRAのつながり

LoRA (Low-Rank Adaptation) は、大規模モデルの重みを直接微調整する代わりに「低ランク行列」を追加学習する手法であり、その発想は特異値分解 (Singular Value Decomposition; SVD) の視点で捉えると直観的に理解しやすい。ここでは LoRA の全体像と SVD との関係を一望できるメモを整理する。

## 1. LoRA 概要
- 事前学習済みの重み \(W_0 \in \mathbb{R}^{d_{\text{out}} \times d_{\text{in}}}\) を固定（凍結）したまま、新たに低ランクの補正 \(\Delta W\) を加えることでモデルを適応させる。
- \(\Delta W\) を Rank \(r \ll \min(d_{\text{in}}, d_{\text{out}})\) の行列分解で表すことで、追加パラメータ数と計算コストを大幅に削減する。
- 推論時は補正を吸収した \(W = W_0 + \Delta W\) をそのまま使えるため、学習時のみ余計な計算を気にすればよい。

## 2. 特異値分解 (SVD) 基礎
- 任意の行列 \(W\) は \(W = U \Sigma V^\top\) と分解できる。
  - \(U \in \mathbb{R}^{d_{\text{out}} \times d_{\text{out}}}\)、\(V \in \mathbb{R}^{d_{\text{in}} \times d_{\text{in}}}\) は直交行列。
  - \(\Sigma\) は特異値を対角成分にもつ \(d_{\text{out}} \times d_{\text{in}}\) の対角行列。
- 特異値の大きい上位 \(r\) 個だけを残した近似 \(W \approx U_r \Sigma_r V_r^\top\) は、情報を極力保ちながら低ランクで表現できる。

## 3. SVD 視点で見る LoRA
- SVD の「上位 \(r\) 次元だけで行列を近似する」発想を、そのまま学習中に適用したものが LoRA と捉えられる。
- LoRA の補正行列は \(\Delta W = B A\) として扱うのが一般的（実装では \(A \in \mathbb{R}^{r \times d_{\text{in}}}\)、\(B \in \mathbb{R}^{d_{\text{out}} \times r}\)）。
- 学習で最も重要な変化量は、SVD のように実は低次元で表現できるのでは？という仮説に基づき、効率の良い更新を実現する。

## 4. LoRA の大まかな流れ
1. **対象レイヤーの選定**  
   - 多くの場合、アテンションの `W_q`、`W_k`、`W_v`、`W_o` や FFN の `W_1`、`W_2` に LoRA アダプタを挿入する。
2. **ベース重みを凍結**  
   - 事前学習済み重み \(W_0\) は更新しない。Gradient は LoRA アダプタのみに流れる。
3. **低ランクアダプタの初期化**  
   - \(A\) をランダム初期化、\(B\) をゼロ初期化しておき、学習初期にベース重みを乱さないようにする。
4. **タスクデータで微調整**  
   - \(\Delta W = \frac{\alpha}{r} B A\) のスケーリング付き更新を通して、低ランク部分のみを最適化する。
5. **推論・デプロイ**  
   - 推論時は \(W = W_0 + \Delta W\) を合成して通常の線形層として扱う（あるいは重みを事前にマージしておく）。

## 5. 直観を掴むミニまとめ
- SVD が示す「多くの行列は低ランクで近似できる」という事実を、学習の差分に適用したのが LoRA。
- 高価な全パラメータ更新ではなく、変化の本質だけを抽出することで効率化している。
- Rank \(r\) を上げれば表現力は上がるがコストも増えるため、SVD と同様にタスクに応じたバランス設計が重要。
